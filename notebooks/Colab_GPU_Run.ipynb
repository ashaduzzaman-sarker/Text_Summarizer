{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ashaduzzaman-sarker/Text_Summarizer.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phAMe1GK1__H",
        "outputId": "e52edae9-744b-431e-c313-8e6f44ad5e22"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Text_Summarizer'...\n",
            "remote: Enumerating objects: 180, done.\u001b[K\n",
            "remote: Counting objects: 100% (180/180), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 180 (delta 77), reused 145 (delta 45), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (180/180), 12.74 MiB | 7.95 MiB/s, done.\n",
            "Resolving deltas: 100% (77/77), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Text_Summarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KM9gh5E5Tdr",
        "outputId": "63ba3d57-88e7-4a46-e17c-15fd4fe2a5f7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Text_Summarizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R-funRE5TFg",
        "outputId": "4e084e59-de20-4336-cf01-8689c51b2f99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tDockerfile  main.py    project_template.py  README.md\t      src\n",
            "config\tLICENSE     notebooks  pyproject.toml\t    requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies from requirements.txt\n",
        "%pip install -r requirements.txt -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhXSgNGb_rTO",
        "outputId": "c0a2729f-31b2-4e0c-a7fd-4596227727cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.2/466.2 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m148.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.8/438.8 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.0/97.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m429.9/429.9 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.5/375.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m752.6/752.6 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m753.1/753.1 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for textSummarizer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELzp-PTy-gYZ",
        "outputId": "0b940a83-53f3-4537-bd70-dd42abc4245d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-10-25 15:39:40,602: INFO: utils: NumExpr defaulting to 2 threads.]\n",
            "[2025-10-25 15:39:41,025: INFO: config: TensorFlow version 2.19.0 available.]\n",
            "[2025-10-25 15:39:41,026: INFO: config: JAX version 0.7.2 available.]\n",
            "2025-10-25 15:39:47.341099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1761406787.371578    2523 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1761406787.380922    2523 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1761406787.404471    2523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761406787.404513    2523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761406787.404522    2523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1761406787.404529    2523 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-25 15:39:47.411274: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[2025-10-25 15:39:51,502: INFO: main: ============================================================]\n",
            "[2025-10-25 15:39:51,503: INFO: stage_01_data_ingestion: >>>>>> Stage: Data Ingestion started <<<<<<]\n",
            "[2025-10-25 15:39:51,505: INFO: common: Loaded config from config/config.yaml]\n",
            "[2025-10-25 15:39:51,507: INFO: common: Loaded config from config/params.yaml]\n",
            "[2025-10-25 15:39:51,508: INFO: common: Created directory: artifacts]\n",
            "[2025-10-25 15:39:51,508: INFO: common: Created directory: artifacts/data_ingestion]\n",
            "[2025-10-25 15:39:51,508: INFO: data_ingestion: Loading dataset: abisee/cnn_dailymail]\n",
            "[2025-10-25 15:39:51,508: INFO: data_ingestion: Config: 3.0.0, Split: train]\n",
            "[2025-10-25 15:39:56,097: INFO: data_ingestion: Limited dataset: 287113 -> 100 samples]\n",
            "[2025-10-25 15:39:56,097: INFO: data_ingestion: Dataset loaded: 100 samples]\n",
            "[2025-10-25 15:39:56,097: INFO: data_ingestion: Features: ['article', 'highlights', 'id']]\n",
            "Saving the dataset (1/1 shards): 100% 100/100 [00:00<00:00, 30246.66 examples/s]\n",
            "[2025-10-25 15:39:56,102: INFO: data_ingestion: Dataset saved to: artifacts/data_ingestion/dataset]\n",
            "[2025-10-25 15:39:56,102: INFO: stage_01_data_ingestion: >>>>>> Stage: Data Ingestion completed <<<<<<\n",
            "]\n",
            "[2025-10-25 15:39:56,103: INFO: main: ============================================================]\n",
            "[2025-10-25 15:39:56,103: INFO: stage_02_data_validation: >>>>>> Stage: Data Validation started <<<<<<]\n",
            "[2025-10-25 15:39:56,105: INFO: common: Loaded config from config/config.yaml]\n",
            "[2025-10-25 15:39:56,108: INFO: common: Loaded config from config/params.yaml]\n",
            "[2025-10-25 15:39:56,108: INFO: common: Created directory: artifacts]\n",
            "[2025-10-25 15:39:56,108: INFO: common: Created directory: artifacts/data_validation]\n",
            "[2025-10-25 15:39:56,108: INFO: data_validation: Starting data validation]\n",
            "[2025-10-25 15:39:56,108: INFO: data_validation: Dataset directory found: artifacts/data_ingestion/dataset]\n",
            "[2025-10-25 15:39:56,108: INFO: data_validation: Loading dataset from artifacts/data_ingestion/dataset]\n",
            "[2025-10-25 15:39:56,110: INFO: data_validation: Dataset columns: ['article', 'highlights', 'id']]\n",
            "[2025-10-25 15:39:56,110: INFO: data_validation: All required columns present]\n",
            "[2025-10-25 15:39:56,111: INFO: data_validation: Dataset size: 100 samples]\n",
            "[2025-10-25 15:39:56,114: INFO: data_validation: Data quality checks passed]\n",
            "[2025-10-25 15:39:56,114: INFO: data_validation: Validation status written to artifacts/data_validation/status.txt]\n",
            "[2025-10-25 15:39:56,115: INFO: data_validation: All validation checks passed]\n",
            "[2025-10-25 15:39:56,115: INFO: stage_02_data_validation: >>>>>> Stage: Data Validation completed <<<<<<\n",
            "]\n",
            "[2025-10-25 15:39:56,115: INFO: main: ============================================================]\n",
            "[2025-10-25 15:39:56,115: INFO: stage_03_data_transformation: >>>>>> Stage: Data Transformation started <<<<<<]\n",
            "[2025-10-25 15:39:56,117: INFO: common: Loaded config from config/config.yaml]\n",
            "[2025-10-25 15:39:56,120: INFO: common: Loaded config from config/params.yaml]\n",
            "[2025-10-25 15:39:56,120: INFO: common: Created directory: artifacts]\n",
            "[2025-10-25 15:39:56,120: INFO: common: Created directory: artifacts/data_transformation]\n",
            "[2025-10-25 15:39:56,120: INFO: data_transformation: Starting data transformation]\n",
            "[2025-10-25 15:39:56,120: INFO: data_transformation: Loading tokenizer: facebook/bart-large-cnn]\n",
            "config.json: 1.58kB [00:00, 9.78MB/s]\n",
            "vocab.json: 899kB [00:00, 92.2MB/s]\n",
            "merges.txt: 456kB [00:00, 131MB/s]\n",
            "tokenizer.json: 1.36MB [00:00, 153MB/s]\n",
            "[2025-10-25 15:39:58,983: INFO: data_transformation: Tokenizer loaded successfully]\n",
            "[2025-10-25 15:39:58,983: INFO: data_transformation: Loading dataset from artifacts/data_ingestion/dataset]\n",
            "[2025-10-25 15:39:58,987: INFO: data_transformation: Dataset loaded: 100 samples]\n",
            "[2025-10-25 15:39:58,987: INFO: data_transformation: Tokenizing dataset...]\n",
            "Tokenizing: 100% 100/100 [00:00<00:00, 257.30 examples/s]\n",
            "[2025-10-25 15:39:59,407: INFO: data_transformation: Tokenization complete: 100 samples]\n",
            "[2025-10-25 15:39:59,407: INFO: data_transformation: Features: ['input_ids', 'attention_mask', 'labels']]\n",
            "Saving the dataset (1/1 shards): 100% 100/100 [00:00<00:00, 18416.26 examples/s]\n",
            "[2025-10-25 15:39:59,414: INFO: data_transformation: Transformed dataset saved to: artifacts/data_transformation/tokenized_dataset]\n",
            "[2025-10-25 15:39:59,481: INFO: data_transformation: Tokenizer saved to: artifacts/data_transformation/tokenizer]\n",
            "[2025-10-25 15:39:59,481: INFO: stage_03_data_transformation: >>>>>> Stage: Data Transformation completed <<<<<<\n",
            "]\n",
            "[2025-10-25 15:39:59,495: INFO: main: ============================================================]\n",
            "[2025-10-25 15:39:59,495: INFO: stage_04_model_trainer: >>>>>> Stage: Model Training started <<<<<<]\n",
            "[2025-10-25 15:39:59,499: INFO: common: Loaded config from config/config.yaml]\n",
            "[2025-10-25 15:39:59,505: INFO: common: Loaded config from config/params.yaml]\n",
            "[2025-10-25 15:39:59,505: INFO: common: Created directory: artifacts]\n",
            "[2025-10-25 15:39:59,505: INFO: common: Created directory: artifacts/model_trainer]\n",
            "[2025-10-25 15:39:59,506: INFO: model_trainer: Starting model training]\n",
            "[2025-10-25 15:39:59,506: INFO: model_trainer: Loading tokenized dataset from artifacts/data_transformation/tokenized_dataset]\n",
            "[2025-10-25 15:39:59,509: INFO: model_trainer: Dataset loaded: 100 samples]\n",
            "[2025-10-25 15:39:59,509: INFO: model_trainer: Splitting dataset: 90% train]\n",
            "[2025-10-25 15:39:59,515: INFO: model_trainer: Train samples: 90]\n",
            "[2025-10-25 15:39:59,516: INFO: model_trainer: Validation samples: 10]\n",
            "[2025-10-25 15:39:59,516: INFO: model_trainer: Loading tokenizer from artifacts/data_transformation/tokenizer]\n",
            "[2025-10-25 15:39:59,689: INFO: model_trainer: Loading model: facebook/bart-large-cnn]\n",
            "model.safetensors: 100% 1.63G/1.63G [00:30<00:00, 54.1MB/s]\n",
            "generation_config.json: 100% 363/363 [00:00<00:00, 3.78MB/s]\n",
            "[2025-10-25 15:40:31,643: INFO: model_trainer: Model and tokenizer loaded successfully]\n",
            "[2025-10-25 15:40:31,862: INFO: model_trainer: Training arguments configured]\n",
            "[2025-10-25 15:40:31,863: INFO: model_trainer: Total epochs: 1]\n",
            "[2025-10-25 15:40:31,863: INFO: model_trainer: Batch size: 2]\n",
            "[2025-10-25 15:40:31,863: INFO: model_trainer: Learning rate: 5e-05]\n",
            "/content/Text_Summarizer/src/textSummarizer/components/model_trainer.py:149: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "[2025-10-25 15:40:32,539: INFO: model_trainer: Training started...]\n",
            "100% 6/6 [00:39<00:00,  6.05s/it]/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n",
            "{'train_runtime': 111.3286, 'train_samples_per_second': 0.808, 'train_steps_per_second': 0.054, 'train_loss': 8.074204126993815, 'epoch': 1.0}\n",
            "100% 6/6 [01:51<00:00, 18.55s/it]\n",
            "[2025-10-25 15:42:32,765: INFO: model_trainer: Training completed!]\n",
            "[2025-10-25 15:42:32,768: INFO: model_trainer: Final model saved to: artifacts/model_trainer/final_model]\n",
            "[2025-10-25 15:42:32,769: INFO: model_trainer: Training loss: 8.0742]\n",
            "[2025-10-25 15:42:32,769: INFO: model_trainer: Training metrics saved to: artifacts/model_trainer/training_metrics.txt]\n",
            "[2025-10-25 15:42:32,770: INFO: stage_04_model_trainer: >>>>>> Stage: Model Training completed <<<<<<\n",
            "]\n",
            "[2025-10-25 15:42:32,784: INFO: main: ============================================================]\n",
            "[2025-10-25 15:42:32,785: INFO: main: All pipeline stages completed successfully!]\n",
            "[2025-10-25 15:42:32,785: INFO: main: Trained model available at: artifacts/model_trainer/final_model]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c9af1eb5",
        "outputId": "95a7beae-86df-40ad-8828-6c0e6bf898b7"
      },
      "source": [
        "!zip -r /content/artifacts.zip /content/Text_Summarizer/artifacts\n",
        "from google.colab import files\n",
        "files.download('/content/artifacts.zip')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/Text_Summarizer/artifacts/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/config.json (deflated 62%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/model.safetensors (deflated 10%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/merges.txt (deflated 53%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/vocab.json (deflated 59%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/special_tokens_map.json (deflated 85%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/tokenizer_config.json (deflated 73%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/tokenizer.json (deflated 82%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/generation_config.json (deflated 46%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/final_model/training_args.bin (deflated 53%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/training_metrics.txt (deflated 22%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/config.json (deflated 62%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/model.safetensors (deflated 10%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/optimizer.pt (deflated 8%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/trainer_state.json (deflated 56%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/merges.txt (deflated 53%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/vocab.json (deflated 59%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/special_tokens_map.json (deflated 85%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/tokenizer_config.json (deflated 73%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/tokenizer.json (deflated 82%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/rng_state.pth (deflated 26%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/scheduler.pt (deflated 61%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/generation_config.json (deflated 46%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/checkpoints/checkpoint-6/training_args.bin (deflated 53%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/logs/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/model_trainer/logs/events.out.tfevents.1761406832.8885ad095bd6.2523.0 (deflated 62%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/dataset/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/dataset/cache-0d2e7229aa609d22.arrow (deflated 78%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/dataset/data-00000-of-00001.arrow (deflated 60%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/dataset/dataset_info.json (deflated 74%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/dataset/state.json (deflated 38%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d.incomplete_info.lock (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d_builder.lock (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cnn_dailymail-test.arrow (deflated 62%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/dataset_info.json (deflated 70%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cnn_dailymail-train-00001-of-00003.arrow (deflated 62%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cnn_dailymail-train-00002-of-00003.arrow (deflated 62%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cnn_dailymail-validation.arrow (deflated 62%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/abisee___cnn_dailymail/3.0.0/0.0.0/96df5e686bee6baa90b8bee7c28b81fa3fa6223d/cnn_dailymail-train-00000-of-00003.arrow (deflated 61%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_ingestion/cache/artifacts_data_ingestion_cache_abisee___cnn_dailymail_3.0.0_0.0.0_96df5e686bee6baa90b8bee7c28b81fa3fa6223d.lock (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenized_dataset/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenized_dataset/data-00000-of-00001.arrow (deflated 78%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenized_dataset/dataset_info.json (deflated 75%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenized_dataset/cache-f8bcfc85bd5bd95a.arrow (deflated 49%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenized_dataset/state.json (deflated 38%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenized_dataset/cache-75e5ff1fd11c4790.arrow (deflated 64%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenizer/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenizer/merges.txt (deflated 53%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenizer/vocab.json (deflated 59%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenizer/special_tokens_map.json (deflated 52%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenizer/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_transformation/tokenizer/tokenizer.json (deflated 82%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_validation/ (stored 0%)\n",
            "  adding: content/Text_Summarizer/artifacts/data_validation/status.txt (stored 0%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1c029de0-f2ef-4563-998f-9bfa29aa8769\", \"artifacts.zip\", 6462283694)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}